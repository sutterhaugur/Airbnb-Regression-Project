{
 "cells": [
  {
   "cell_type": "raw",
   "id": "b1c13c80",
   "metadata": {},
   "source": [
    "---\n",
    "title: \"Prediction Problem Report (Name of model (KNN / RF / Boosting / Ensemble); Regression / Classification)\"\n",
    "format: \n",
    "  html:\n",
    "    toc: true\n",
    "    toc-title: Contents\n",
    "    toc-depth: 4\n",
    "    code-fold: show\n",
    "    self-contained: true\n",
    "    html-math-method: mathml \n",
    "jupyter: python3\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d4b9670",
   "metadata": {},
   "source": [
    "## Instructions {-}\n",
    "\n",
    "- This is the template for the code and report on the Prediction Problem assignments.\n",
    "\n",
    "- Your code in steps 1, 3, 4, and 5 will be executed sequentially, and must produce the RMSE / accuracy claimed on Kaggle.\n",
    "\n",
    "- Your code in step 2 will also be executed, and must produce the optimal hyperparameter values used to train the model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95e86f05",
   "metadata": {},
   "source": [
    "## Read data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a4298a29",
   "metadata": {},
   "outputs": [],
   "source": [
    "#importing Libraries \n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import statsmodels.formula.api as smf\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.model_selection import GridSearchCV, KFold\n",
    "from sklearn.ensemble import RandomForestRegressor\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "06025554",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = pd.read_csv('train_regression.csv')\n",
    "test_data = pd.read_csv('test_regression.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b5d4b16",
   "metadata": {},
   "source": [
    "## 1) Data pre-processing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8adc06dc",
   "metadata": {},
   "source": [
    "Put the data pre-processing code. You don't need to explain it. You may use the same code from last quarter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7e28d640",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/wt/pbzn3z4n6kq9sxg167nbz_fh0000gn/T/ipykernel_2988/1170753690.py:96: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  train_data['minimum_maximum_nights'][indexes] = 1125\n",
      "/var/folders/wt/pbzn3z4n6kq9sxg167nbz_fh0000gn/T/ipykernel_2988/1170753690.py:149: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  train_data['number_of_reviews_l30d'][indexes] = 70\n"
     ]
    }
   ],
   "source": [
    "train_data['host_response_rate'] = train_data.host_response_rate.apply(lambda x: str(x)).apply(lambda x: x.replace('%','')).apply(lambda x: float(x))\n",
    "\n",
    "#host acceptence rate\n",
    "train_data['host_acceptance_rate'] = train_data.host_acceptance_rate.apply(lambda x: str(x)).apply(lambda x: x.replace('%','')).apply(lambda x: float(x))\n",
    "\n",
    "\n",
    "#dummy variable host_has_profile_pic\n",
    "train_data = pd.concat([train_data, pd.get_dummies(train_data['host_has_profile_pic']).rename(columns={'t': 'host_has_profile_pic_t'})], axis = 1)\n",
    "train_data.drop(labels = ['host_has_profile_pic', 'f'], axis = 1, inplace = True)\n",
    "\n",
    "#dummy variable host_identity_verified\n",
    "train_data = pd.concat([train_data, pd.get_dummies(train_data['host_identity_verified']).rename(columns={'t': 'host_identity_verified_t'})], axis = 1)\n",
    "train_data.drop(labels = ['host_identity_verified', 'f'], axis = 1, inplace = True)\n",
    "\n",
    "#dummy variable host_is_superhost\n",
    "train_data = pd.concat([train_data, pd.get_dummies(train_data['host_is_superhost']).rename(columns={'t': 'host_is_superhost_t'})], axis = 1)\n",
    "train_data.drop(labels = ['host_is_superhost', 'f'], axis = 1, inplace = True)\n",
    "\n",
    "#number of bathrooms\n",
    "train_data['Number_of_Bathrooms'] = train_data.bathrooms_text.apply(lambda x: str(x)).apply(lambda x: x.replace('shared baths','').replace('shared bath','').replace('Shared half-bath', '0.5').replace('Private half-bath', '0.5').replace('Half-bath','0.5').replace('private bath','').replace('baths','').replace('bath','')).apply(lambda x: float(x))\n",
    "\n",
    "#classifying bathrooms as private and public\n",
    "train_data['shared_bathroom'] = train_data['bathrooms_text'].str.contains('shared', case=False)\n",
    "train_data['private_bathroom'] = train_data['bathrooms_text'].str.contains('private', case=False)\n",
    "\n",
    "#dummy variable has_availability\n",
    "train_data = pd.concat([train_data, pd.get_dummies(train_data['has_availability']).rename(columns={'t': 'has_availability_t'})], axis = 1)\n",
    "train_data.drop(labels = ['has_availability', 'f'], axis = 1, inplace = True)\n",
    "\n",
    "#dummy variable instant_bookable\n",
    "train_data = pd.concat([train_data, pd.get_dummies(train_data['instant_bookable']).rename(columns={'t': 'instant_bookable_t'})], axis = 1)\n",
    "train_data.drop(labels = ['instant_bookable', 'f'], axis = 1, inplace = True)\n",
    "\n",
    "#classifying downtown neigborhhods\n",
    "downtown_neighborhoods = ['River North', 'Loop', 'Gold Coast', 'Streeterville', \\\n",
    "                          'South Loop', 'West Loop', 'River West', 'Near North Side']\n",
    "\n",
    "#new column is_downtown\n",
    "train_data['is_downtown']= train_data.host_neighbourhood.isin(downtown_neighborhoods).astype(int)\n",
    "\n",
    "#defining a fucntion to classify a row as other if it is not in the top 42 frequencies of neigborhood\n",
    "def reclassify(row):\n",
    "    if row['neighbourhood_cleansed'] not in (list(train_data.neighbourhood_cleansed.value_counts()[0:42].index)):\n",
    "        row['neighbourhood_cleansed'] = 'other'\n",
    "    else:\n",
    "        row\n",
    "    return(row)\n",
    "\n",
    "#applying the function to train\n",
    "train_data = train_data.apply(reclassify, axis = 1)\n",
    "neigborhood_dummies = pd.get_dummies(train_data['neighbourhood_cleansed'])\n",
    "neigborhood_dummies.columns = neigborhood_dummies.columns.str.replace(' ', '_')\n",
    "train_data = pd.concat([train_data, neigborhood_dummies], axis =1)\n",
    "\n",
    "\n",
    "#converting the datetime columns to numeric\n",
    "train_data['host_since'] = (pd.Timestamp.now() - pd.to_datetime(train_data.host_since)).dt.days\n",
    "train_data['first_review'] = (pd.Timestamp.now() - pd.to_datetime(train_data.first_review)).dt.days\n",
    "train_data['last_review'] = (pd.Timestamp.now() - pd.to_datetime(train_data.last_review)).dt.days\n",
    "\n",
    "#hostresponse time dummies\n",
    "host_response_time_dummies = pd.get_dummies(train_data['host_response_time'])\n",
    "host_response_time_dummies.columns = host_response_time_dummies.columns.str.replace(' ', '_')\n",
    "train_data = pd.concat([train_data, host_response_time_dummies], axis =1)\n",
    "\n",
    "#host_verification dummies\n",
    "host_verifications_dummies = pd.get_dummies(train_data['host_verifications'])\n",
    "host_verifications_dummies.columns = host_verifications_dummies.columns.str.replace(' ', '_')\n",
    "new_columns = ['email_workemail_phone', 'email_phone', 'email', 'phone_workemail', 'phone']\n",
    "host_verifications_dummies.columns = new_columns\n",
    "train_data = pd.concat([train_data, host_verifications_dummies], axis =1)\n",
    "\n",
    "#room type dummies\n",
    "room_type_dummies = pd.get_dummies(train_data['room_type'])\n",
    "room_type_dummies.columns = room_type_dummies.columns.str.replace(' ', '_')\n",
    "train_data = pd.concat([train_data, room_type_dummies], axis =1)\n",
    "\n",
    "#defining a function to reclassify property_types as other of its not in to top twenty freqnecies\n",
    "def reclassify_ptype(row):\n",
    "    if row['property_type'] not in list(train_data.property_type.value_counts()[:20].index):\n",
    "        row['property_type'] = 'other'\n",
    "    else:\n",
    "        row\n",
    "    return(row)\n",
    "\n",
    "#applying the function \n",
    "train_data = train_data.apply(reclassify_ptype, axis = 1)\n",
    "\n",
    "#prop type dummies\n",
    "property_type_dummies = pd.get_dummies(train_data['property_type'])\n",
    "property_type_dummies.columns = property_type_dummies.columns.str.replace(' ', '_')\n",
    "train_data = pd.concat([train_data, property_type_dummies], axis =1)\n",
    "\n",
    "#reassigning an extreme value of minimum_maximum_nights to 1125\n",
    "indexes = list(train_data.minimum_maximum_nights[train_data.minimum_maximum_nights == train_data.minimum_maximum_nights.max()].index)\n",
    "train_data['minimum_maximum_nights'][indexes] = 1125\n",
    "\n",
    "#defining a function to reclassify a host neigborhood if its not in. the top 66 frequencies of neigborhoods\n",
    "def reclassify_host_n(row):\n",
    "    if row['host_neighbourhood'] not in list(train_data.host_neighbourhood.value_counts()[:66].index):\n",
    "        row['host_neighbourhood'] = 'other'\n",
    "    else:\n",
    "        row\n",
    "    return(row)\n",
    "\n",
    "#applying the function across the data\n",
    "train_data = train_data.apply(reclassify_host_n, axis = 1)\n",
    "\n",
    "#dummie variables for host_neighbourhood\n",
    "h_neighbourhood_dummies = pd.get_dummies(train_data['host_neighbourhood'])\n",
    "h_neighbourhood_dummies.columns = 'is_' + h_neighbourhood_dummies.columns.str.replace(' ', '_').str.replace('/', '_')\n",
    "train_data = pd.concat([train_data, h_neighbourhood_dummies], axis =1)\n",
    "\n",
    "train_data['price'] = train_data.price.apply(lambda x: x.replace('$','').replace(',','')).apply(lambda x: float(x))\n",
    "\n",
    "#getting rid of outliers\n",
    "train_data = train_data.loc[train_data.price < 80000]\n",
    "\n",
    "#take log\n",
    "indexes = list(train_data.host_listings_count[train_data.host_listings_count > 50].index)\n",
    "train_data['host_listings_count'][indexes] = 50\n",
    "\n",
    "#take log\n",
    "indexes = list(train_data.host_total_listings_count[train_data.host_total_listings_count > 200].index)\n",
    "train_data['host_total_listings_count'][indexes] = 200\n",
    "\n",
    "\n",
    "indexes = list(train_data.beds[train_data.beds > 15].index)\n",
    "train_data['beds'][indexes] = 15\n",
    "\n",
    "indexes = list(train_data.minimum_minimum_nights[train_data.minimum_minimum_nights > 200].index)\n",
    "train_data['minimum_minimum_nights'][indexes] = 200\n",
    "\n",
    "indexes = list(train_data.minimum_nights_avg_ntm[train_data.minimum_nights_avg_ntm > 400].index)\n",
    "train_data['minimum_nights_avg_ntm'][indexes] = 400\n",
    "\n",
    "#take log\n",
    "indexes = list(train_data.number_of_reviews[train_data.number_of_reviews > 300].index)\n",
    "train_data['number_of_reviews'][indexes] = 300\n",
    "\n",
    "#take log\n",
    "indexes = list(train_data.number_of_reviews_ltm[train_data.number_of_reviews_ltm > 70].index)\n",
    "train_data['number_of_reviews_ltm'][indexes] = 70\n",
    "\n",
    "indexes = list(train_data.number_of_reviews_l30d[train_data.number_of_reviews_l30d > 70].index)\n",
    "train_data['number_of_reviews_l30d'][indexes] = 70\n",
    "\n",
    "indexes = list(train_data.number_of_reviews_l30d[train_data.number_of_reviews_l30d > 70].index)\n",
    "train_data['number_of_reviews_l30d'][indexes] = 70\n",
    "\n",
    "\n",
    "indexes = list(train_data.first_review[train_data.first_review < 0].index)\n",
    "train_data['first_review'][indexes] = 125\n",
    "\n",
    "\n",
    "indexes = list(train_data.last_review[train_data.last_review < 0].index)\n",
    "train_data['last_review'][indexes] = 155\n",
    "\n",
    "\n",
    "indexes = list(train_data.calculated_host_listings_count[train_data.calculated_host_listings_count > 300].index)\n",
    "train_data['calculated_host_listings_count'][indexes] = 95\n",
    "\n",
    "indexes = list(train_data.calculated_host_listings_count_entire_homes[train_data.calculated_host_listings_count_entire_homes > 300].index)\n",
    "train_data['calculated_host_listings_count_entire_homes'][indexes] = 95\n",
    "\n",
    "#take log \n",
    "indexes = list(train_data.reviews_per_month[train_data.reviews_per_month > 10].index)\n",
    "train_data['reviews_per_month'][indexes] = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e3ce9aad",
   "metadata": {},
   "outputs": [],
   "source": [
    "#host_response_rate\n",
    "test_data['host_response_rate'] = test_data.host_response_rate.apply(lambda x: str(x)).apply(lambda x: x.replace('%','')).apply(lambda x: float(x))\n",
    "\n",
    "#host acceptence rate\n",
    "test_data['host_acceptance_rate'] = test_data.host_acceptance_rate.apply(lambda x: str(x)).apply(lambda x: x.replace('%','')).apply(lambda x: float(x))\n",
    "\n",
    "#dummy variable host_has_profile_pic\n",
    "test_data = pd.concat([test_data, pd.get_dummies(test_data['host_has_profile_pic']).rename(columns={'t': 'host_has_profile_pic_t'})], axis = 1)\n",
    "test_data.drop(labels = ['host_has_profile_pic', 'f'], axis = 1, inplace = True)\n",
    "\n",
    "#dummy variable host_identity_verified\n",
    "test_data = pd.concat([test_data, pd.get_dummies(test_data['host_identity_verified']).rename(columns={'t': 'host_identity_verified_t'})], axis = 1)\n",
    "test_data.drop(labels = ['host_identity_verified', 'f'], axis = 1, inplace = True)\n",
    "\n",
    "test_data = pd.concat([test_data, pd.get_dummies(test_data['host_is_superhost']).rename(columns={'t': 'host_is_superhost_t'})], axis = 1)\n",
    "test_data.drop(labels = ['host_is_superhost', 'f'], axis = 1, inplace = True)\n",
    "\n",
    "#number of bathrooms\n",
    "test_data['Number_of_Bathrooms'] = test_data.bathrooms_text.apply(lambda x: str(x)).apply(lambda x: x.replace('shared baths','').replace('shared bath','').replace('Shared half-bath', '0.5').replace('Private half-bath', '0.5').replace('Half-bath','0.5').replace('private bath','').replace('baths','').replace('bath','')).apply(lambda x: float(x))\n",
    "\n",
    "#classifying bathrooms as private and public\n",
    "test_data['shared_bathroom'] = test_data['bathrooms_text'].str.contains('shared', case=False)\n",
    "test_data['private_bathroom'] = test_data['bathrooms_text'].str.contains('private', case=False)\n",
    "\n",
    "#dummy variable has_availability\n",
    "test_data = pd.concat([test_data, pd.get_dummies(test_data['has_availability']).rename(columns={'t': 'has_availability_t'})], axis = 1)\n",
    "test_data.drop(labels = ['has_availability', 'f'], axis = 1, inplace = True)\n",
    "\n",
    "#dummy variable instant_bookable\n",
    "test_data = pd.concat([test_data, pd.get_dummies(test_data['instant_bookable']).rename(columns={'t': 'instant_bookable_t'})], axis = 1)\n",
    "test_data.drop(labels = ['instant_bookable', 'f'], axis = 1, inplace = True)\n",
    "\n",
    "\n",
    "\n",
    "#classifying downtown neigborhhods\n",
    "downtown_neighborhoods = ['River North', 'Loop', 'Gold Coast', 'Streeterville', \\\n",
    "                          'South Loop', 'West Loop', 'River West', 'Near North Side']\n",
    "\n",
    "#new column is_downtown\n",
    "test_data['is_downtown']= test_data.host_neighbourhood.isin(downtown_neighborhoods).astype(int)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#applying the function reclassify to test\n",
    "#getting dummies for neighbourhood_cleansed\n",
    "test_data = test_data.apply(reclassify, axis = 1)\n",
    "neigborhood_dummies_test = pd.get_dummies(test_data['neighbourhood_cleansed'])\n",
    "neigborhood_dummies_test.columns = neigborhood_dummies_test.columns.str.replace(' ', '_')\n",
    "test_data = pd.concat([test_data, neigborhood_dummies_test], axis =1)\n",
    "\n",
    "#converting the datetime columns to numeric\n",
    "test_data['host_since'] = (pd.Timestamp.now() - pd.to_datetime(test_data.host_since)).dt.days\n",
    "test_data['first_review'] = (pd.Timestamp.now() - pd.to_datetime(test_data.first_review)).dt.days\n",
    "test_data['last_review'] = (pd.Timestamp.now() - pd.to_datetime(test_data.last_review)).dt.days\n",
    "\n",
    "\n",
    "#hostresponse time dummies\n",
    "host_response_time_dummies_test = pd.get_dummies(test_data['host_response_time'])\n",
    "host_response_time_dummies_test.columns = host_response_time_dummies_test.columns.str.replace(' ', '_')\n",
    "test_data = pd.concat([test_data, host_response_time_dummies_test], axis =1)\n",
    "\n",
    "#host_verification dummies\n",
    "host_verifications_dummies_test = pd.get_dummies(test_data['host_verifications'])\n",
    "host_verifications_dummies_test.columns = host_verifications_dummies_test.columns.str.replace(' ', '_')\n",
    "new_columns = ['email_workemail_phone', 'email_phone', 'email', 'phone_workemail', 'phone']\n",
    "host_verifications_dummies_test.columns = new_columns\n",
    "test_data = pd.concat([test_data, host_verifications_dummies_test], axis =1)\n",
    "\n",
    "#room type dummies\n",
    "room_type_dummies_test = pd.get_dummies(test_data['room_type'])\n",
    "room_type_dummies_test.columns = room_type_dummies_test.columns.str.replace(' ', '_')\n",
    "test_data = pd.concat([test_data, room_type_dummies_test], axis =1)\n",
    "\n",
    "#applying the function reclassify_ptype\n",
    "test_data = test_data.apply(reclassify_ptype, axis = 1)\n",
    "\n",
    "#prop type dummies\n",
    "property_type_dummies_test = pd.get_dummies(test_data['property_type'])\n",
    "property_type_dummies_test.columns = property_type_dummies_test.columns.str.replace(' ', '_')\n",
    "test_data = pd.concat([test_data, property_type_dummies_test], axis =1)\n",
    "\n",
    "#applying the function reclassify_host_n across the data\n",
    "test_data = test_data.apply(reclassify_host_n, axis = 1)\n",
    "\n",
    "#dummie variables for host_neighbourhood\n",
    "h_neighbourhood_dummies_test = pd.get_dummies(test_data['host_neighbourhood'])\n",
    "h_neighbourhood_dummies_test.columns = 'is_' + h_neighbourhood_dummies_test.columns.str.replace(' ', '_').str.replace('/', '_')\n",
    "test_data = pd.concat([test_data, h_neighbourhood_dummies_test], axis =1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f21715b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.impute import KNNImputer\n",
    "imputer = KNNImputer(n_neighbors=5)\n",
    "\n",
    "test_data_numeric = test_data.select_dtypes(include = ['int64', 'float64', 'uint8'])\n",
    "train_data_numeric = train_data.select_dtypes(include = ['int64', 'float64', 'uint8'])\n",
    "\n",
    "test_data_numeric = pd.DataFrame(imputer.fit_transform(test_data_numeric),columns = test_data_numeric.columns)\n",
    "train_data_numeric = pd.DataFrame(imputer.fit_transform(train_data_numeric),columns = train_data_numeric.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8bd63d3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import Ridge, RidgeCV, Lasso, LassoCV, LogisticRegressionCV, LogisticRegression\n",
    "from sklearn.preprocessing import StandardScaler \n",
    "from sklearn.metrics import r2_score, accuracy_score\n",
    "from sklearn.model_selection import cross_val_score, cross_val_predict\n",
    "\n",
    "train_data_numeric.drop(labels = ['Belmont_Cragin', 'Private_room_in_guest_suite', 'is_East_Colorado_Springs'], axis = 1, inplace = True )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7df3f99e",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = train_data_numeric.drop(columns = ['id', 'host_id', 'price'], axis = 1)\n",
    "y = train_data_numeric.price\n",
    "X_test = test_data_numeric.drop(columns = ['id', 'host_id'], axis = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "730eaefd",
   "metadata": {},
   "source": [
    "## 2) Hyperparameter tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f9b39b7",
   "metadata": {},
   "source": [
    "### How many attempts did it take you to tune the model hyperparameters?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4e33d51",
   "metadata": {},
   "source": [
    "Around 10 attempts to tune the hyperparameters"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a6f50fd",
   "metadata": {},
   "source": [
    "### Which tuning method did you use (grid search / Bayes search / etc.)?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43ea3666",
   "metadata": {},
   "source": [
    "Grid Search"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a0da667",
   "metadata": {},
   "source": [
    "### What challenges did you face while tuning the hyperparameters, and what actions did you take to address those challenges?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbe8149e",
   "metadata": {},
   "source": [
    "I did not face many major challenges while tuning hyperparameters. I did have difficulty decided weather or not to select variables from the data set. I ultimatly decided not to use variable selection."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f26daac",
   "metadata": {},
   "source": [
    "### How many hours did you spend on hyperparameter tuning?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d8149d3",
   "metadata": {},
   "source": [
    "45 minutes to an hour"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ba4abb9",
   "metadata": {},
   "source": [
    "**Paste the hyperparameter tuning code below. You must show at least one hyperparameter tuning procedure.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efccf837",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Hyperparameter tuning code\n",
    "from sklearn.model_selection import RandomizedSearchCV, KFold\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "kf = KFold(n_splits = 5, shuffle = True)\n",
    "\n",
    "model = RandomForestRegressor(oob_score=True, verbose=False, n_jobs=-1)\n",
    "\n",
    "params = {'n_estimators': np.arange(100,3000),\n",
    "          'max_features': np.arange(1,66),\n",
    "         'max_samples': np.arange(0.01,1.01,0.01)}\n",
    "\n",
    "\n",
    "gcv = RandomizedSearchCV(model, params, n_iter = 60, cv = kf, scoring = 'neg_root_mean_squared_error', n_jobs = -1)\n",
    "gcv.fit(X_train,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "38cbdbbb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'max_features': 42, 'max_samples': 0.94, 'n_estimators': 100}\n",
      "-133.63987461942853\n"
     ]
    }
   ],
   "source": [
    "print(gcv.best_params_)\n",
    "print(gcv.best_score_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10fc357d",
   "metadata": {},
   "source": [
    "**Paste the optimal hyperparameter values below.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccf5c031",
   "metadata": {},
   "source": [
    "{'max_features': 42, 'max_samples': 0.94}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e104de7",
   "metadata": {},
   "source": [
    "## 3) Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50a37864",
   "metadata": {},
   "source": [
    "Using the optimal model hyperparameters, train the model, and paste the code below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a6462944",
   "metadata": {},
   "outputs": [],
   "source": [
    "# When I ran the code earlier I got 0.9 and 43, so I used these hyperparameters\n",
    "\n",
    "model = RandomForestRegressor(oob_score=True, verbose=False, \\\n",
    "    max_features = 43, max_samples = 0.9, n_estimators = 400, \\\n",
    "                    n_jobs=-1, random_state =45).fit(X_train,y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "897d6954",
   "metadata": {},
   "source": [
    "## 4) Put any ad-hoc steps for further improving model accuracy\n",
    "For example, scaling up or scaling down the predictions, capping predictions, etc.\n",
    "\n",
    "Put code below."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cee5ca73",
   "metadata": {},
   "source": [
    "No ad-hoc steps"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec1c5d42",
   "metadata": {},
   "source": [
    "## 5) Export the predictions in the format required to submit on Kaggle\n",
    "Put code below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "31202106",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>predicted</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>771986218856585018</td>\n",
       "      <td>348.5500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>855276028675941785</td>\n",
       "      <td>133.8025</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>48537824</td>\n",
       "      <td>309.2675</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>41867473</td>\n",
       "      <td>135.8550</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>28361473</td>\n",
       "      <td>103.0850</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3333</th>\n",
       "      <td>22399540</td>\n",
       "      <td>147.3650</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3334</th>\n",
       "      <td>964949640914649520</td>\n",
       "      <td>72.9875</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3335</th>\n",
       "      <td>18007859</td>\n",
       "      <td>88.7050</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3336</th>\n",
       "      <td>736269020394606618</td>\n",
       "      <td>102.3475</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3337</th>\n",
       "      <td>39872019</td>\n",
       "      <td>150.5225</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3338 rows Ã— 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                      id  predicted\n",
       "0     771986218856585018   348.5500\n",
       "1     855276028675941785   133.8025\n",
       "2               48537824   309.2675\n",
       "3               41867473   135.8550\n",
       "4               28361473   103.0850\n",
       "...                  ...        ...\n",
       "3333            22399540   147.3650\n",
       "3334  964949640914649520    72.9875\n",
       "3335            18007859    88.7050\n",
       "3336  736269020394606618   102.3475\n",
       "3337            39872019   150.5225\n",
       "\n",
       "[3338 rows x 2 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prices = model.predict(X_test)\n",
    "df = pd.concat([test_data['id'],pd.Series(prices)], axis = 1)\n",
    "\n",
    "\n",
    "df.columns = ['id', 'predicted']\n",
    "df"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
